{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on the available kaggle competition - Detection of toxic comment.  \n",
    "link: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import necessary packages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import string\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import Imputer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=pd.read_csv('datasets/tow_classes_tweet.csv', encoding='utf-8', usecols=['sentiment', 'txt'])\n",
    "dataset1=pd.read_csv('C:/Users/aas3n17/Desktop/Organized/datasets/Ar_tweet_cleaned_final.csv', encoding='utf-8', usecols=['sentiment', 'txt'])\n",
    "dataset2=pd.read_csv('C:/Users/aas3n17/Desktop/Organized/datasets/qrci.csv', encoding='utf-8', usecols=['sentiment', 'txt'])\n",
    "dataset3=pd.read_csv('C:/Users/aas3n17/Desktop/Organized/datasets/astd.csv', encoding='utf-8', usecols=['sentiment', 'txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    993\n",
       "0    958\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    377\n",
       "0    377\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    812\n",
       "1    777\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=clean(dataset1)\n",
    "dataset2=clean(dataset2)\n",
    "dataset3=clean(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Ø¹Ù† Ø§ÙŠ Ù…Ø´Ø±ÙˆØ¨ ØºØ§Ø²ÙŠ ÙƒÙˆï»» Ø­Ø§ÙØ¸Ù…Ø´ÙØ§Ù‡Ù…  \n",
       "1     Ø¹Ø§Ø¯Ù‡ Ù„Ø§ Ø§Ø¹Ù„Ù† Ø¹Ù† Ù…ÙˆØ§Ù‚ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠÙ‡Ù„ÙƒÙ† ÙƒÙ„Ù‡ Ø§Ù„Ø§ Ø§Ù„Ø¯...\n",
       "2     Ø§Ù†ØªØ§ ÙØ±Ø­Ù‡ Ø¬Ø§Øª Ù„ Ø¹Ù†Ø¯ÙŠ Ø¨Ø¹Ø¯ Ø¯Ù†ÙŠØ§ Ù…Ù† Ø§Ù„ØªØ¹Ø¨ Ø§Ù„Ø³Ø¹Ø§Ø¯Ù‡...\n",
       "3     Ø§Ù„Ø¨Ù„Ø§ÙˆÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø§Ø®ÙˆØ§Ù† ØªÙ†Ø¸ÙŠÙ…Ø§ Ø§Ø±Ù‡Ø§Ø¨ÙŠØ§ Ù‚Ø±Ø§Ø± Ø³ÙŠØ§Ø³...\n",
       "4      ÙÙƒØ±Ù‡ Ø§Ù„Ù…Ù‚Ø§Ù„Ù‡ Ø­Ù„ÙˆÙ‡ ÙˆØ§Ø³Ù„ÙˆØ¨Ùƒ ÙÙŠ ØªÙˆØµÙŠÙ„ Ø§Ù„ÙÙƒØ±Ù‡ Ø­Ù„Ùˆ...\n",
       "5     Ø§Ù‡Ø§Ù„ÙŠ Ø¹Ø²Ø¨Ù‡Ø§Ù„Ù†Ø®Ù„ ÙŠÙØ¶Ø­ÙˆÙ† Ø§Ù„Ø³ÙŠØ³ÙŠ Ø­Ù…Ù„ØªÙ‡ Ù‚Ø§Ù„Øª Ù„Ù†Ø§ Ø§...\n",
       "6     Ø§Ù„Ø³Ø§Ø¯Ù‡ Ø§Ù„Ø§ÙØ§Ø¶Ù„ Ø¨Ø¬Ø±ÙŠØ¯Ù‡ Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† Ø§Ù„Ø®Ø¨Ø± Ø§Ù„...\n",
       "7     Ø§Ù„Ø­Ù…Ø¯ Ù„Ù‡ Ø§Ù† Ø­Ù‚Ù† Ø¯Ù…Ø§Ø¡ Ø§Ù„Ù…ØµØ±ÙŠÙ† ÙˆØ§ØµÙ„Ø­ Ø¨ÙŠÙ†Ù‡Ù… ÙˆØ¹Ù‚Ø¨Ø§...\n",
       "8     ÙŠØ§ Ù…Ø±Ø³ÙŠ ÙŠØ§ Ø¹Ø¸ÙŠÙ… ØªØ³Ø±ÙŠØ¨ Ø³ÙŠØ³ÙŠ ÙƒØ´Ù Ø§Ù†Ù‡ ÙƒØ§Ù† Ø¹Ù…Ù„Ø§Ù‚Ø§ ...\n",
       "9     Ø§Ù„Ø¹Ø¨ÙŠØ¯ ÙÙŠ Ø§Ø¹Ù„Ø§Ù… Ø§Ù„Ø¹Ø§Ø± Ù„Ù… ÙŠÙ†Ø·Ù‚ Ø¨Ø¹Ø¯ ØªØµØ±ÙŠØ­ Ø§ÙŠÙ‡Ø§Ø¨ ...\n",
       "10    Ø§Ø³Ø±Ø§Ø± Ù†Ø´Ø§Ù‡ Ø§Ù„Ø³ÙŠØ³ÙŠ Ø¯Ø§Ø®Ù„ Ø­Ø§Ø±Ù‡ Ø§Ù„ÙŠÙ‡ÙˆØ¯ Ø¨Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡ Ø§Ù„...\n",
       "11    Ù„Ø¨ÙŠÙƒ Ø§Ù„Ù‡Ù… Ù„Ø¨ÙŠÙƒ Ù„Ø¨ÙŠÙƒ Ù„Ø§ Ø´Ø±ÙŠÙƒ Ù„Ùƒ Ù„Ø¨ÙŠÙƒ Ø§Ù† Ø§Ù„Ø­Ù…Ø¯ Ùˆ...\n",
       "12    Ù†Ø­Ù† Ø§Ù…Ù‡ Ø§Ø°Ø§ ÙˆØ¶Ø¹Øª Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„Ø§Ù†ØªØ±Ù†Øª ÙÙŠÙ‡Ø§ ÙƒÙ„Ù…Ù‡ Ù...\n",
       "13    Ø§Ù„Ù‡Ù… Ø§Ù…Ø·Ø± Ø¹Ù„ÙŠ Ù‚Ø¨Ø± Ù…Ø­Ù…Ø¯ Ø­Ø³ÙŠÙ† Ø¬Ù…Ø¹Ù‡ ÙˆØ¹Ù„ÙŠ Ù‚Ø¨ÙˆØ± Ø¬Ù…ÙŠ...\n",
       "14    Ø­ÙÙŠØ¯ Ø§Ù„Ø³Ù„Ø·Ø§Ù† Ø¹Ø¨Ø¯Ø§Ù„Ø­Ù…ÙŠØ¯ Ù…Ø±Ø´Ø­ Ù„Ø¨Ø±Ù„Ù…Ø§Ù† Ø§Ù„ØªØ±ÙƒÙŠ Ø§Ù„Ù‚...\n",
       "15    Ù…Ù† Ø§Ø¬Ù…Ù„ Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ø·ÙÙˆÙ„Ù‡ Ø§Ù†Ùƒ ØªÙ†Ø§Ù… ÙÙŠ Ø§ÙŠ Ù…ÙƒØ§Ù† ÙÙŠ ...\n",
       "16    Ø§Ù„Ù…ØµØ±ÙŠ Ø¨ÙŠÙ‚ÙˆÙ„ Ø­Ø²Ø§Ù… Ù†Ø§Ø³Ù Ùˆ Ø§Ù„Ø´Ø±ÙˆÙ‚ Ø¨ÙŠÙ‚ÙˆÙ„ Ø³ÙŠØ§Ø±Ù‡ Ù…Ø³...\n",
       "17    Ø¹ Ù…Ù†Ø·Ù‚ Ø­Ù‚ÙŠØ± ÙˆÙ„Ø§ ÙŠØªØ¨Ø¹Ù‡ Ø³ÙˆÙŠ Ø§Ù„Ø¬Ø¨Ù†Ø§Ø¡ Ù‡Ø°Ù‡ Ù†ÙØ³ ØªÙ‚ØªÙ„...\n",
       "18    Ø¨Ø¹Ø¯ Ù…ØµØ§Ø¯Ø±Ù‡ ÙÙŠÙ„Ù… Ø­Ù„Ø§ÙˆÙ‡ Ø±ÙˆØ­ Ù…Ù†Ø¹ Ø³Ù…Ø§Ù„Ù…ØµØ±ÙŠ Ù…Ù† Ø§Ù„ØºÙ†...\n",
       "19                                         Ø´ÙƒØ±Ø§ Ù„Ø¯Ø¹Ù…Ùƒ  \n",
       "20    Ø§Ø³ØªÙØªØ§Ø¡ Ù…Ø§Ø±Ø³ Ù…Ø§Ù†ÙØ¹Ø´ Ø·Ù†Ø·Ø§ÙˆÙŠ ÙˆÙ‚Ø¯ ÙƒØ§Ù† Ø§ÙƒØ«Ø± Ø´Ø±ÙØ§ Ùˆ...\n",
       "21    ØµØ§Ø­Ø¨ Ù‚Ù†Ø§Ù‡ Ø§Ù„ÙØ±Ø§Ø¹ÙŠÙ† Ù„Ù…ØµØ±ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù†Ø§ Ù…Ù„Ùƒ Ø§Ù„ØªÙˆÙƒ Ø´...\n",
       "22    Ù†Ø¹Ù… Ø§Ù†Ù‡ Ø®Ø§Ù„Ø¯Ø¨Ù†Ø§Ù„ÙˆÙ„ÙŠØ¯ Ø±Ø¶ÙŠ Ø§Ù„Ù‡ Ø¹Ù†Ù‡ ÙŠØ§ Ù…Ù† ÙŠÙØªØ®Ø± Ø¨...\n",
       "23    Ø±Ø­Ù… Ø§Ù„Ù‡ Ø§Ù„Ø§Ø³ØªØ§Ø° Ø³Ù„Ø§Ù…Ù‡ Ø§Ø­Ù…Ø¯ Ø³Ù„Ø§Ù…Ù‡ØŒ Ù‚Ù„Ù… Ø§Ù„Ø¶Ù…ÙŠØ± Ùˆ...\n",
       "24    Ø§Ø¹ØªØ°Ø§Ø± Ø³ÙŠ Ø¨ÙŠ Ø³ÙŠ Ø¹Ù† Ø­Ù„Ù‚Ù‡ Ø§Ù„Ø´ÙŠØ·Ø§Ù† Ø§Ù„Ø§Ø±Ø§Ø¬ÙˆØ² Ø¬Ø²Ø¡ Ù…...\n",
       "25     ØªØ¯Ø¹ÙˆÙ† Ù„Ø¯ÙŠÙ† ÙˆØªÙ†Ø´Ø±ÙˆÙ† Ø§Ù„Ø§ÙƒØ§Ø°ÙŠØ¨ØŒ Ù„Ù… Ø§ÙƒÙ† Ø§Ø¨Ø¯Ø§ Ø¹Ø¶ÙˆØ§...\n",
       "26    ÙƒØ§Ù†ÙˆØ§ Ø±Ø¬Ø§Ù„ Ù…Ø±Ø³ÙŠØ±Ø¡ÙŠØ³ÙŠ ÙƒØ§Ù†ÙˆØ§Ø±Ø¬Ø§Ù„ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ±Ø¯ØªØ­ÙŠÙ‡Ø§Ù„Ø±...\n",
       "27    Ø§Ù„ÙŠ Ø­Ù…Ø¯ÙŠÙ†ØµØ¨Ø§Ø­ÙŠ Ù„Ùˆ Ø§Ù†Øª Ø¹Ø§ÙˆØ² ØªØ­Ø§ÙƒÙ… Ø§Ù„Ø³ÙŠØ³ÙŠ ÙŠØ¨Ù‚ÙŠ Ø§...\n",
       "28    Ø§Ù„Ù‚Ø§ÙƒÙ… Ø¨Ø¹Ø¯ Ø±Ø¨Ø¹ Ø³Ø§Ø¹Ù‡ Ù…Ù† Ø§Ù„Ø§Ù† ÙÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø­ÙŠØ§Ù‡ Ø§...\n",
       "29    Ø§Ø±Ø¬Ùˆ Ø§Ù„Ø¯Ø¹Ø§Ø¡ Ù„Ø±Ø§Ø­Ù„ Ø§Ù„Ø¹Ø¸ÙŠÙ… Ù…Ø­Ù…ÙˆØ¯ Ø§Ù„Ø³Ø¹Ø¯Ù†ÙŠ Ø§Ù„ÙŠÙˆÙ… Ø°...\n",
       "                            ...                        \n",
       "70    ÙˆØ§Ø­Ø¯ Ø¨ÙŠØ·Ø¨Ù„ Ø¨Ù‚Ø§Ù„Ù‡ Ø³Ù†ÙŠÙ† ÙˆÙÙŠ Ø§Ù„Ø§Ø®Ø± Ù…Ø´ Ø¹Ø§ÙŠØ²ÙŠÙ†Ù‡ ÙŠØ¨Ù‚...\n",
       "71                                      Ø§Ø´ÙƒØ±Ùƒ ÙŠØ§ Ø®Ø§Ù„Ø¯  \n",
       "72    Ø¨ ÙˆØ¨Ø¹Ø¯ Ø¹Ø²Ù„ Ø§Ø¬Ø¨Ø§Ø±ÙŠ Ù„Ø±Ø¡ÙŠØ³ Ø§Ù„Ù…Ø¯Ù†ÙŠ Ø§Ù„Ù…Ù†ØªØ®Ø¨ ÙƒÙ†Øª Ø§ØªÙ…...\n",
       "73    Ø§ØªÙ…Øª Ù…Ù‡Ù…ØªÙŠ ÙƒØ¹Ø¶Ùˆ Ù„Ø¬Ù†Ù‡ ØªØ­ÙƒÙŠÙ… ÙÙŠ Ù…Ù‡Ø±Ø¬Ø§Ù† ÙÙŠÙ†ÙŠØ³ÙŠØ§ Ø§...\n",
       "74    ÙˆØ¬Ù‡ Ù†Ø¸Ø± Ø³Ø§Ø°Ø¬Ù‡ ØªÙ‚ÙˆÙ„ Ø§Ù† Ù…Ù† Ù†Ø§Ø¶Ù„ ÙÙŠ Ø§Ù„Ù…Ø§Ø¶ÙŠ ÙŠØ­Ù‚ Ù„Ù‡...\n",
       "75    Ø³ÙŠØ§Ø¯Ù‡ Ø§Ù„Ø±Ø¡ÙŠØ³ØŒ Ø§Ù†Ø§ Ø§Ø­ØªØ±Ù…Ùƒ Ø¨Ù‚Ø¯Ø± Ø§Ø­ØªØ±Ø§Ù…Ùƒ Ù„Ù…Ø­ÙƒÙ…Ù‡ Ø§...\n",
       "76    ÙƒÙ†Øª ØªØ§Ø¬Ù„ ÙˆØªÙŠØ¬ÙŠ Ù…Ù†Ùƒ Ø¨Ø¬Ù…ÙŠÙ„Ù‡ ÙˆØªØ¹Ù…Ù„ ÙÙŠÙ‡Ø§ Ù‚Ø¹Ø± Ù…Ø¬Ù„Ø³ ...\n",
       "77    Ø³ØªØ¸Ù„ Ø°ÙƒØ±ÙŠ Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø§Ù„Ø¨Ø·Ù„ Ø¹Ø¨Ø¯ Ø§Ù„Ù…Ù†Ø¹Ù… Ø±ÙŠØ§Ø¶ ØªØ®Ù„ÙŠØ¯Ø§...\n",
       "78    Ù„Ù… Ø§ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„ Ø®Ù„Ø§Ù„ Ø§Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ø§Ø¶ÙŠÙ‡ Ø¹Ù„ÙŠ Ø§Ù„Ø§Ø·Ù„Ø§Ù‚...\n",
       "79     Ø§Ù„Ø³Ø¨Ø§Ø­Ù‡ Ø§Ø²Ø§ÙŠ Ø§Ù„Ø³ÙŠØ§Ø­ ÙŠØ¬ÙˆØ§ Ùˆ Ù…Ø­Ø§ÙØ¸ Ø§Ù„Ø§Ù‚ØµØ± Ø§Ø±Ù‡Ø§Ø¨ÙŠ  \n",
       "80                                       Ø§Ù†Øª Ø¨Ø¶Ø§Ù† ÙŠØ§Ø¶  \n",
       "81    ØµØ¯Ù‚ Ù…Ù† Ù‚Ø§Ù„ Ù„Ø§ØªØ³ØªØ·ÙŠØ¹ Ø´Ø±Ø§Ø¡ Ø§Ù„Ø§Ø®ÙˆØ§Ù† Ù„Ø§Ù†Ù‡Ù… Ù‚ÙˆÙ… Ù„Ø§Ø³...\n",
       "82    Ø¨Ù„Ø·Ø¬Ù‡ Ø§Ù„Ø§Ø®ÙˆØ§Ù† ÙˆØ§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¹Ù„ÙŠ Ø§Ù„Ø§Ø¹Ù„Ø§Ù…ÙŠÙ† Ø¨Ù…Ø­ÙŠØ· Ø§Ùƒ...\n",
       "83                  Ù…ØµØ± Ø§Ù„ÙŠÙˆÙ… Ø­Ø±Ù‡ Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù‡ ÙÙŠ Ø´Ø¹Ø¨ Ù…ØµØ±  \n",
       "84    Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ ÙÙ„ØªØ± Ø¨ÙŠÙ† Ù…Ø®ÙŠ ÙˆÙ„Ø³Ø§Ù†ÙŠ Ø¯ÙŠ Ù…ØµÙŠØ¨Ù‡ Ø§Ù„ Ø¨ÙÙƒØ± Ù...\n",
       "85      Ù„Ù…ØµØ± Ø´Ø¹Ø¨ ÙŠØ­Ù…ÙŠÙ‡Ø§ Ø±Ø¨ Ø§Ø¬Ø¹Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ù„Ø¯ Ø§Ù…Ù†Ø§ Ù…Ø·Ù…Ø¡Ù†Ø§  \n",
       "86    Ù†Ø­Ø¨ Ø§Ù„ØºÙ‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡ Ù„Ø§Ù†Ù‡Ø§ Ù„ØºÙ‡ Ø§Ù„Ù‚Ø±Ø§Ù† ÙˆÙ„Ø§Ù† Ù†Ø¨ÙŠÙ†Ø§ Ø¹...\n",
       "87    Ø´ÙØª ÙƒÙ„ Ø´ÙŠ Ø¨Ø§Ù„Ø­ÙŠØ§Ù‡ Ø¨Ø³ Ø§ÙˆÙ„ Ù…Ø±Ù‡ Ø§Ø´ÙˆÙ Ø´Ø¨ Ø­Ø§Ù…Ù„ Ø·Ø¨Ù„Ù‡...\n",
       "88    Ù…ÙØ§Ø¬Ø§Ù‡ Ø±Ø¯ Ø§Ù„Ø´ÙŠØ® Ø­Ø§Ø²Ù… Ø¹Ù„ÙŠ ÙØªÙˆÙŠ Ø¨Ø±Ù‡Ø§Ù…ÙŠ Ø§Ù„Ø§Ø®ÙŠØ±Ù‡ Ù...\n",
       "89     Ù†Ø¨Ø¶Ø§Ù„Ø§Ø®ÙˆØ§Ù† Ø§ Ø¹ØµØ§Ù…Ø³Ù„Ø·Ø§Ù† Ùˆ Ø«Ø¨Ø§Øª Ø¹Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø¯Ø§ Ø§Ø­Ù†Ø§...\n",
       "90    Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø¹ÙŠØ¯ ØªØ­Ø±ÙŠØ± Ø³ÙŠÙ†Ø§ Ø§Ù„Ø³ÙŠØ¯ Ø§Ù„Ø±Ø¡ÙŠØ³ Ø§Ù„Ø³Ø§Ø¯Ù‡ ØµÙ†Ø§...\n",
       "91                                     ÙˆØ¨Ø­Ø¨Ùƒ ÙŠØ§ Ø²Ù…Ø§Ù„Ùƒ  \n",
       "92    Ø§Ù„Ù…Ø§Ø¸Ù‡Ù„Ø§Ø¨Ø¯ Ù…Ù† Ù…Ø¹Ø§ÙŠØ±Ù‡ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ¹Ø±ÙŠÙÙ‡ ...\n",
       "93    Ø§Ù†Ø§ Ù„Ùˆ Ù†ÙØ³ÙŠ Ø§Ø®Ø· Ùˆ Ø§Ø±ØªØ¨ Ùˆ Ø§ØµØ±Ù ÙÙ„ÙˆØ³ Ø§Ù„Ø¯Ù†ÙŠØ§ Ø¹Ø´Ø§Ù†...\n",
       "94    Ù†Ø²Ù„Ù†Ø§ Ù†Ø§ÙŠØ¯Ùƒ ÙˆÙ…ÙÙŠØ´ Ø­Ø§Ø¬Ù‡ Ø§ØªØºÙŠØ±Øª ÙˆØ¨Ù‚ÙŠÙ†Ø§ Ù…Ø³Ø®Ù‡ Ù„Ø¹Ø§Ù„...\n",
       "95                            ÙØ±ÙŠÙ‚ Ø§Ù„ÙˆØ¹Ø¯ Ø§Ù†ØµØ§ÙØ§Ù„Ø§Ø®ÙˆØ§Ù†  \n",
       "96    Ø§ÙˆÙ„ Ù…Ø±Ù‡ Ø§Ø´ÙˆÙ Ù…Ù†Ø§Ø¸Ø±Ù‡ Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø§Ø·Ø¹Ù‡ ÙˆÙ„Ø§ ØµØ±Ø§Ø® ÙˆÙƒÙ„ Ø·...\n",
       "97    ÙŠÙˆÙ…Ø§ Ù…Ø§ Ø³ÙŠØ¯Ø±Ø³ Ø§ÙˆÙ„Ø§Ø¯Ù†Ø§ Ø´Ø¬Ø§Ø¹Ù‡ Ø§Ù„Ù…Ø±Ø§Ù‡ Ø§Ù„Ù…ØµØ±ÙŠÙ‡ Ø§Ù„Øª...\n",
       "98    Ø§Ø±ÙŠ Ø§Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· Ø¨ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠ ÙˆØ§Ù„...\n",
       "99    Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù‡ ÙˆØ§Ù„Ø­Ù…Ø¯ Ù„Ù‡ ÙˆÙ„Ø§ Ø§Ù„Ù‡ Ø§Ù„Ø§ Ø§Ù„Ù‡ ÙˆÙ„Ø§ Ø­ÙˆÙ„ ÙˆÙ„...\n",
       "Name: txt, Length: 100, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.txt.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.1)\n",
    "train.head()\n",
    "#prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little preprocessing required\n",
    "sentences_train = train[\"txt\"].fillna(\"_na_\").values\n",
    "classes = [\"sentiment\"]\n",
    "y = train[classes].values\n",
    "sentences_test = test[\"txt\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding parameter set\n",
    "embed_size = 100 # how big is each word vector\n",
    "max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(sentences_train))\n",
    "tokens_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "tokens_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "X_train = pad_sequences(tokens_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(tokens_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 4)             1680      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 47, 16)            272       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 752)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               75300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,077,353\n",
      "Trainable params: 1,077,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(4, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)(x)\n",
    "x = Conv1D(16,4,activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.rmsprop(lr = 0.001,decay = 1e-06), metrics=['accuracy'])\n",
    "filepath=\"Weights/weights-improvement.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the defined model onto the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1404 samples, validate on 351 samples\n",
      "Epoch 1/5\n",
      "1404/1404 [==============================] - 3s 2ms/step - loss: 0.6678 - acc: 0.5848 - val_loss: 0.6238 - val_acc: 0.5897\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58974, saving model to Weights/weights-improvement.hdf5\n",
      "Epoch 2/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.4524 - acc: 0.8027 - val_loss: 0.4049 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58974 to 0.81197, saving model to Weights/weights-improvement.hdf5\n",
      "Epoch 3/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.2513 - acc: 0.9010 - val_loss: 0.3310 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81197 to 0.85470, saving model to Weights/weights-improvement.hdf5\n",
      "Epoch 4/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.1458 - acc: 0.9459 - val_loss: 0.3378 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85470\n",
      "Epoch 5/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.0833 - acc: 0.9708 - val_loss: 0.3920 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85470 to 0.85755, saving model to Weights/weights-improvement.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ecd1d33b38>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y, batch_size=32, epochs=5,callbacks=callbacks_list, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select from saved weights as per choice and predict response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model('Weights/weights-improvement.hdf5')\n",
    "y_pred = saved_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred.round()\n",
    "y_test=test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8342494714587738, 0.813953488372093, 0.8235294117647058, 0.8045977011494253]\n"
     ]
    }
   ],
   "source": [
    "        f1_score = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "        # macro accuracy (macro average)\n",
    "        macc = metrics.f1_score(y_test, y_pred, pos_label=None, average='macro')\n",
    "\n",
    "        # precision and recall\n",
    "        recall = metrics.recall_score(y_test, y_pred)\n",
    "        precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "        results = [macc, f1_score, precision, recall]\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clean(df):\n",
    "        path = \"Arabic_stop_word.txt\"\n",
    "        stop_words = []\n",
    "        with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "            stop_words = myfile.readlines()\n",
    "        stop_words = [word.strip() for word in stop_words]\n",
    "        arabic_punctuations = '''Ø›<>_()*ØŒ&^%][Ù€ØŒ/:\"ØŸÙ€`Ã·Ã—.,'{}~Â¦+|!â€â€¦â€œâ€“_'''\n",
    "        arabic_numbers = '''Û°Û±Û²Û³Ù¤Ù¥Ù¦Ù§Û¸Û¹'''\n",
    "        english_punctuations = string.punctuation\n",
    "        punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "        arabic_diacritics = re.compile(\"\"\"\n",
    "                             Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "        for comments in df:\n",
    "            # removearabic_diacritics\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(arabic_diacritics,'',str(x)))\n",
    "\n",
    "            #def normalize_arabic(text)\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Ø¤\", \"Ø¡\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Ù‰\", \"ÙŠ\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Ø¦\", \"Ø¡\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Ø©\", \"Ù‡\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Ú¯\", \"Ùƒ\",str(x)))\n",
    "\n",
    "            \n",
    "\n",
    "            #def remove_punctuations(text):\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"[\"+punctuations_list+\"]\",'',str(x)))\n",
    "\n",
    "            # remove_repeating_char(text):\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(r'(.)\\1+', r'\\1', str(x)))\n",
    "            # remove '\\\\n'\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "\n",
    "            # remove any text starting with User... \n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "\n",
    "            # remove IP addresses or user IDs\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "\n",
    "            # lower uppercase letters\n",
    "            #df['txt'] = df['txt'].map(lambda x: str(x).lower())\n",
    "\n",
    "            #remove http links in the text\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "            #remove all punctuation\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"_\", '',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Â«\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Â»\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â€œ\", ' ',str(x))) \n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â€\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸ˜\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸ˜”\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸ˜‚\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸŒ¹\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"âœ¨\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸ‘‰\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸ‘ˆ\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â˜¹\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ğŸ‘‡\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Ú©\", 'Ùƒ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â€”\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"áƒš\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â•¹\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â—¡\", ' ',str(x)))\n",
    "            \n",
    "            \n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â™¥\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"â™¡\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"Â¥\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ØŸ\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"!\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"#\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"$\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"%\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"&\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"âœ–\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Ø©', 'Ù‡',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(' Ø¦', 'Ø¡',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Ø¤', 'Ø¡',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Û’', 'Ùƒ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Úª', 'Ùƒ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Ø£', 'Ø§',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Ø¥', 'Ø§',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('Ø¢', 'Ø§',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('\\\\r',' ',str(x)))\n",
    "            df['txt']= df['txt'].map(lambda x: re.sub(\"[\"+string.punctuation+\"]\",'',str(x)))\n",
    "            df['txt']= df['txt'].map(lambda x: re.sub(\"[\"+arabic_numbers+\"]\",'',str(x)))\n",
    "            df['txt']= df['txt'].map(lambda x: re.sub(\"[\"+punctuations_list+\"]\",'',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"[\"+digits+\"]\",'',str(x)))\n",
    "            #df['txt'] = df['txt'].map(lambda x: re.sub(r'[^\\x600-\\x6ff]','',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(r'[a-zA-Z?]','',str(x))) \n",
    "            df['txt'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
